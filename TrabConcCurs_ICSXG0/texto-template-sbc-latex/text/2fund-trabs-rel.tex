\section{Fundamentos e Trabalhos Relacionados}

Esta seção apresenta os principais conceitos e trabalhos relacionados que fundamentam o desenvolvimento deste estudo, com foco em LLMs, técnicas de fine-tuning e métodos de explicabilidade aplicados à resolução de labirintos em ASCII.

\subsection{LLMs - Large Language Models}

Os Modelos de Linguagem de Grande Escala (\textit{Large Language Models} - LLMs) são arquiteturas baseadas em redes neurais profundas, predominantemente do tipo \textit{transformer}, projetadas para processar e gerar linguagem natural. Esses modelos são treinados com grandes volumes de dados textuais, aprendendo padrões estatísticos da linguagem humana e tornando-se capazes de executar uma ampla gama de tarefas, muitas vezes sem necessidade de treinamento supervisionado específico, como sumarização, resposta a perguntas e tradução. O tamanho dos LLMs, geralmente medido em bilhões de parâmetros, está diretamente associado à sua capacidade de adaptação a diferentes contextos e à generalização. A arquitetura \textit{transformer}, introduzida por \cite{vaswani2023transformers}, é central para esses modelos, destacando-se pelo uso de mecanismos de atenção que permitem a modelagem de dependências contextuais de longo alcance.

\subsection{Fine-tuning}

O \textit{fine-tuning} é uma técnica fundamental para adaptar LLMs a tarefas específicas, por meio do re-treinamento de parte ou da totalidade dos parâmetros do modelo com conjuntos de dados direcionados. Após o pré-treinamento, em que o LLM aprende representações gerais da linguagem, o fine-tuning permite especializar o modelo para contextos delimitados, como a navegação em labirintos ASCII, foco deste trabalho. Entre as principais estratégias de fine-tuning destacam-se:

\begin{itemize}
    \item \textbf{Supervised Fine-Tuning (SFT)}: ajuste supervisionado com dados rotulados, comum em tarefas como classificação e tradução.
    \item \textbf{Direct Preference Optimisation (DPO)}: otimização baseada em preferências humanas, visando respostas mais alinhadas ao usuário.
    \item \textbf{Reinforcement Learning from Human Feedback (RLHF)}: aprendizado por reforço com recompensas baseadas em feedback humano.
    \item \textbf{Odds Ratio Preference Optimization (ORPO)}: otimização baseada na razão de chances entre respostas preferidas e não preferidas.
    \item \textbf{Group Relative Policy Optimization (GRPO)}: ajuste considerando preferências coletivas de grupos de usuários.
\end{itemize}

Essas estratégias são relevantes para o presente estudo, pois permitem investigar como diferentes métodos de ajuste impactam o desempenho dos LLMs em tarefas de raciocínio espacial.

\subsection{Explicabilidade em LLMs}

Apesar do alto desempenho, LLMs são frequentemente criticados por seu caráter de ``caixa-preta'', dificultando o entendimento sobre como tomam decisões. Nesse contexto, técnicas de explicabilidade (\textit{explainability}) têm ganhado destaque, buscando tornar os mecanismos internos dos modelos mais transparentes. O objetivo dessas técnicas é revelar como os dados são processados ao longo das camadas, quais neurônios são ativados em resposta a determinadas entradas e como essas ativações influenciam as saídas e evoluem durante o treinamento ou fine-tuning.

A explicabilidade é essencial para diagnosticar falhas, enviesamentos e limitações dos modelos. Diversas ferramentas têm sido propostas, incluindo visualizações, análise de atenção e uso de grafos de conhecimento, que auxiliam na compreensão do comportamento dos LLMs.

\subsubsection{LLM-MRI}

A biblioteca LLM-MRI (\textit{Large Language Model - Magnetic Resonance Imaging}) é uma ferramenta desenvolvida para facilitar a análise de padrões de ativação em LLMs baseados em \textit{transformer}. Conforme apresentado por \cite{costa2024llmmri}, a LLM-MRI permite coletar, organizar e projetar em dimensões reduzidas os vetores de ativação gerados pelos modelos ao processarem diferentes entradas. Técnicas de redução de dimensionalidade possibilitam visualizar como as ativações neuronais se distribuem, facilitando a identificação de padrões e a análise das mudanças provocadas por diferentes estratégias de fine-tuning.

No contexto deste trabalho, a LLM-MRI é empregada para investigar como LLMs se adaptam à tarefa de navegação em labirintos ASCII e como as estruturas internas dos modelos evoluem durante o processo de aprendizagem, contribuindo para a análise da interpretabilidade e do raciocínio espacial dos modelos.
\subsection{Trabalhos Relacionados}

Esta seção apresenta a revisão de literatura sobre o tema deste trabalho, com foco em LLMs, fine-tunning e explicabilidade.

\subsubsection{Raciocínio Espacial em LLMs}

O raciocínio espacial em LLMs tem sido explorado por diferentes abordagens, buscando adaptar esses modelos para tarefas que exigem compreensão e manipulação de ambientes estruturados, como labirintos em ASCII. O AlphaMaze \cite{dao2025alphamaze} propõe um processo em duas etapas: inicialmente, o modelo é ajustado via Supervised Fine-Tuning (SFT) para aprender comandos de movimentação em labirintos textuais; em seguida, utiliza-se o Group Relative Policy Optimization (GRPO) para aprimorar o raciocínio e a autocorreção. O MazeBench é empregado para avaliar o desempenho dos modelos em diferentes níveis de dificuldade, evidenciando ganhos após o GRPO. Em paralelo, \cite{jiang2024supervised} questionam a vantagem de modelos multimodais em tarefas espaciais, mostrando que LLMs baseados apenas em texto podem superar alternativas multimodais, o que reforça a relevância de investigar estratégias específicas para LLMs textuais. Em contextos tridimensionais, \cite{zhang2025pointvisiontextpointcloudboost} identificam limitações dos LLMs, enquanto \cite{cheng2024spatialrgpt} propõem integrar módulos de representação de regiões e informações de profundidade para melhorar a compreensão espacial. Outras abordagens, como o Chain-of-Symbol (COS) Prompting \cite{hu2024chainofsymbol}, convertem descrições em linguagem natural para representações simbólicas intermediárias, otimizando o planejamento espacial e reduzindo o uso de tokens. Já o Visualization of Thought (VoT) \cite{wu2024visualizationofthought} estimula a geração de representações visuais em ASCII art, inspirando-se na cognição humana para melhorar o acompanhamento de estados e o planejamento de ações. Embora o VoT apresente avanços em determinados cenários, sua eficácia depende das habilidades emergentes dos modelos e da qualidade dos prompts, sendo limitado em tarefas mais complexas ou com modelos menos robustos.

No contexto deste trabalho, o raciocínio espacial é central, pois o objetivo é investigar como LLMs podem ser ajustados e analisados para resolver labirintos representados em ASCII. As abordagens discutidas na literatura fornecem subsídios para a escolha de técnicas de fine-tuning e avaliação, além de motivar o uso de ferramentas de explicabilidade para compreender como as representações internas dos modelos evoluem ao longo do treinamento. Dessa forma, este trabalho busca contribuir para o entendimento dos mecanismos de raciocínio espacial em LLMs, avaliando tanto o desempenho quanto a interpretabilidade dos modelos em tarefas estruturadas de navegabilidade em labirintos textuais.

\subsubsection{Fine-tuning de LLMs para Tarefas Específicas}

O fine-tuning de LLMs para tarefas específicas é um tema amplamente investigado, com diferentes estratégias sendo propostas para aprimorar a adaptação dos modelos. Por exemplo, \cite{wang2024pictureworththousandwords} demonstra que o uso de SFT pode melhorar tanto as representações internas quanto a capacidade de generalização, mesmo em modelos visuais, evidenciando a importância do ajuste direcionado para tarefas de raciocínio espacial. Além disso, \cite{hsieh2023distillingstepbystep} propõem a utilização de rationales gerados por LLMs como supervisão adicional em um cenário multitarefa, combinando rótulos e explicações para enriquecer o sinal de treinamento. Essa abordagem, baseada em Chain-of-Thought prompting \cite{wei2023chainofthought}, permite que modelos menores alcancem desempenho superior a LLMs maiores, mesmo com menos dados, ao tornar o processo de ajuste mais eficiente e interpretável. No contexto deste trabalho, a integração de racionalizações se mostra relevante para aprimorar a eficiência e a interpretabilidade dos modelos em tarefas como a resolução de labirintos ASCII.

\subsubsection{Explicabilidade em LLMs}

A explicabilidade em LLMs é um campo em expansão, com abordagens que buscam tornar os modelos mais transparentes e compreensíveis. A biblioteca LLM-MRI \cite{costa2024llmmri} destaca-se por permitir a análise e visualização das ativações neuronais, facilitando a interpretação de como os modelos se adaptam a tarefas específicas, como a navegação em labirintos ASCII. Complementarmente, \cite{wu2025usablexai10strategies} propõem o conceito de Usable XAI, que enfatiza a aplicação prática das explicações para diagnóstico e aprimoramento dos modelos. Entre as estratégias discutidas estão métodos de atribuição, análise das ativações internas e explicações baseadas em exemplos, todas relevantes para entender a evolução das representações internas durante o fine-tuning. A integração dessas abordagens contribui para identificar limitações, corrigir vieses e aprimorar o desempenho dos modelos em tarefas que exigem alto grau de interpretabilidade.