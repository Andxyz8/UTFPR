\section{Fundamentos e Trabalhos Relacionados}

Esta seção discute os trabalhos relacionados e os fundamentos teóricos principais relacionados com o tema proposto.

\subsection{LLMs - Large Language Models}

Os modelos de Linguagem de Grande Escala (\textit{Large Language Models} - \textit{LLMs}) são arquiteturas computacionais baseadas em redes neurais profundas, predominantemente do tipo \textit{transformer}, desenvolvidos para processar e gerar linguagem natural. Esses modelos são treinados com grandes volumes de dados textuais, com os quais conseguem (computar/aprender) padrões estatísticos da linaguagem humana, tornando-os capazes de realizar uma ampla gama de tarefas, grande parte sem treinamento supervisionado específico (e.g. sumarização, resposta a perguntas e tradução). A característica comumente associada ao tamanho desses modelos é uma medida em bilhções de parâmetros que está diretamente associado à sua capacidade de adaptação a contextos diversos e sua generalização. A arquitetura \textit{transformer}, introduzida por \cite{vaswani2023transformers}, faz parte da estrutura central da maioria dos LLMs atuais, e destaca-se pelo uso de mecanismos de atenção que possibilitam a modelagem de dependências contextuais de longo alcance. 

\subsection{Fine-tuning}

O \textit{fine-tunning} é uma técnica utilizada para adaptar \textit{LLMs} a tarefas específicas, por meio de re-treinamento de parte ou totalidade de seus parâmetros com conjuntos de dados mais específicos. Em seguida da etapa inicial de pré-treinamento, na qual um \textit{LLM} aprende as representações gerais linguagem a partir de grandes corpora, o \textit{fine-tunning} permite especializar o comportamento do modelo para contextos mais delimitados como, por exemplo, geração de código, classificação de sentimentos ou, como no presente trabalho, navegação por labirinto em caracteres ASCII. Existem diferentes estratégias de \textit{fine-tunning}:
\begin{itemize}
    \item Supervised Fine-Tuning (SFT): técnica de ajuste fino supervisionado, onde o modelo é treinado com um conjunto de dados rotulado, permitindo que aprenda a mapear entradas para saídas específicas. Essa abordagem é comum para tarefas como classificação, tradução e resposta a perguntas.
    \item Direct Preference Optimisation (DPO): técnica de otimização direta de preferências, onde o modelo é ajustado para maximizar a probabilidade de gerar respostas preferidas por humanos, com base em feedback direto. Essa abordagem é útil para melhorar a qualidade das respostas geradas pelo modelo.
    \item Reinforcement Learning from Human Feedback (RLHF): técnica de aprendizado por reforço a partir de feedback humano, onde o modelo é treinado para maximizar uma recompensa baseada em preferências humanas. Essa abordagem é eficaz para melhorar a qualidade das respostas geradas pelo modelo, alinhando-as com as expectativas dos usuários.
    \item Odds Ratio Preference Optimization (ORPO): técnica de otimização de preferências baseada em razão de chances, onde o modelo é ajustado para maximizar a razão entre as probabilidades de gerar respostas preferidas e não preferidas. Essa abordagem é útil para melhorar a qualidade das respostas geradas pelo modelo, alinhando-as com as expectativas dos usuários. 
    \item Group Relative Policy Optimization (GRPO): técnica de otimização de políticas relativas em grupo, onde o modelo é ajustado para maximizar a probabilidade de gerar respostas preferidas por um grupo de usuários, levando em consideração as preferências coletivas. Essa abordagem é eficaz para melhorar a qualidade das respostas geradas pelo modelo, alinhando-as com as expectativas de um público mais amplo.
\end{itemize}

\subsection{Explicabilidade em LLMs}

Apesar de possuírem um alto nível de compreensão e resolução daquilo que lhes são atribuídos, os LLMs são questionados com frequência sobre seu caráter de ``caixa-preta'', que é a dificuldade no entendimento sobre como tomam decisões ou gram suas respostas.

Nesse contexto, é cada vez maior o interesse em técnicas de explicabilidade (\textit{explainability}) aplicadas a redes neurais profundas para tornar seus mecanismos internos interpretáveis. A explicabilidade em LLMs tem o objetivo de revelar como os dados são processados ao longo das camadas do modelo, quais neurônios foram ativados em resposta a determinadas entradas, como essas ativações influenciam a saída final e como as representações internas evoluem durante o treinamento ou \textit{fine-tunning}.

Essas técnicas são essenciais para o diagnóstico de falhas, enviesamento e limitações de raciocínio do modelo. Diversas ferramentas vêm sendo propostas para auxiliar na explicabilidade de LLMs com visualizações, utilização de grafos de conhecimento, análise de atenção e outras abordagens que ajudam a decifrar o comportamento dos modelos.

\subsubsection{LLM-MRI}

A biblioteca LLM-MRI (\textit{Large Language Model - Magnetic Resonance Imaging}) é uma das ferramentas desenvolvidas com o intuito de auxiliar na explicabilidade de LLMs. Ela atua com o objetivo de simplificar o estudo de padrões de ativações em qualquer LLM baseado em trasnformer.

Desenvolvida por \cite{costa2024llmmri}, a LLM-MRI permite a coleta, organização e projeção em dimensões reduzidas dos vetores de ativação produzidos pelos modelos ao processarem diferentes entradas. Por meio de técnicas de redução de dimensionalidade, é possível visualizar como as ativações neuronais se distribuem em um espaço de menor dimensão, facilitando a identificação de padrões e também como as ativações se diferenciam após a aplicação de diferentes técnicas de fine-tunning.

Essa abordagem é relevante, considerando o contexto experimental deste trabalho, avaliar como LLMs se adaptam a tarefas específicas (neste caso, navegação em labirintos ASCII) e como as estruturas internas dos modelos emergem a partir dessa aprendizagem.

\subsection{Trabalhos Relacionados}

Esta seção apresenta a revisão de literatura sobre o tema deste trabalho, com foco em LLMs, fine-tunning e explicabilidade.

\subsubsection{Raciocínio Espacial em LLMs}

O raciocínio espacial em LLMs tem sido explorado por diferentes abordagens, buscando adaptar esses modelos para tarefas que exigem compreensão e manipulação de ambientes estruturados, como labirintos em ASCII. O AlphaMaze \cite{dao2025alphamaze} propõe um processo em duas etapas: inicialmente, o modelo é ajustado via Supervised Fine-Tuning (SFT) para aprender comandos de movimentação em labirintos textuais; em seguida, utiliza-se o Group Relative Policy Optimization (GRPO) para aprimorar o raciocínio e a autocorreção. O MazeBench é empregado para avaliar o desempenho dos modelos em diferentes níveis de dificuldade, evidenciando ganhos após o GRPO. Em paralelo, \cite{jiang2024supervised} questionam a vantagem de modelos multimodais em tarefas espaciais, mostrando que LLMs baseados apenas em texto podem superar alternativas multimodais, o que reforça a relevância de investigar estratégias específicas para LLMs textuais. Em contextos tridimensionais, \cite{zhang2025point} identificam limitações dos LLMs, enquanto \cite{cheng2024spatialrgpt} propõem integrar módulos de representação de regiões e informações de profundidade para melhorar a compreensão espacial. Outras abordagens, como o Chain-of-Symbol (COS) Prompting \cite{hu2023chain}, convertem descrições em linguagem natural para representações simbólicas intermediárias, otimizando o planejamento espacial e reduzindo o uso de tokens. Já o Visualization of Thought (VoT) \cite{wu2024mind} estimula a geração de representações visuais em ASCII art, inspirando-se na cognição humana para melhorar o acompanhamento de estados e o planejamento de ações. Embora o VoT apresente avanços em determinados cenários, sua eficácia depende das habilidades emergentes dos modelos e da qualidade dos prompts, sendo limitado em tarefas mais complexas ou com modelos menos robustos.

No contexto deste trabalho, o raciocínio espacial é central, pois o objetivo é investigar como LLMs podem ser ajustados e analisados para resolver labirintos representados em ASCII. As abordagens discutidas na literatura fornecem subsídios para a escolha de técnicas de fine-tuning e avaliação, além de motivar o uso de ferramentas de explicabilidade para compreender como as representações internas dos modelos evoluem ao longo do treinamento. Dessa forma, este trabalho busca contribuir para o entendimento dos mecanismos de raciocínio espacial em LLMs, avaliando tanto o desempenho quanto a interpretabilidade dos modelos em tarefas estruturadas de navegabilidade em labirintos textuais.

\subsubsection{Fine-tuning de LLMs para Tarefas Específicas}

O fine-tuning de LLMs para tarefas específicas é um tema amplamente investigado, com diferentes estratégias sendo propostas para aprimorar a adaptação dos modelos. Por exemplo, \cite{wang2024picture} demonstra que o uso de SFT pode melhorar tanto as representações internas quanto a capacidade de generalização, mesmo em modelos visuais, evidenciando a importância do ajuste direcionado para tarefas de raciocínio espacial. Além disso, \cite{hsieh2023distillingstepbystep} propõem a utilização de rationales gerados por LLMs como supervisão adicional em um cenário multitarefa, combinando rótulos e explicações para enriquecer o sinal de treinamento. Essa abordagem, baseada em Chain-of-Thought prompting \cite{wei2023chainofthought}, permite que modelos menores alcancem desempenho superior a LLMs maiores, mesmo com menos dados, ao tornar o processo de ajuste mais eficiente e interpretável. No contexto deste trabalho, a integração de racionalizações se mostra relevante para aprimorar a eficiência e a interpretabilidade dos modelos em tarefas como a resolução de labirintos ASCII.

\subsubsection{Explicabilidade em LLMs}

A explicabilidade em LLMs é um campo em expansão, com abordagens que buscam tornar os modelos mais transparentes e compreensíveis. A biblioteca LLM-MRI \cite{costa2024llmmri} destaca-se por permitir a análise e visualização das ativações neuronais, facilitando a interpretação de como os modelos se adaptam a tarefas específicas, como a navegação em labirintos ASCII. Complementarmente, \cite{wu2025usablexai10strategies} propõem o conceito de Usable XAI, que enfatiza a aplicação prática das explicações para diagnóstico e aprimoramento dos modelos. Entre as estratégias discutidas estão métodos de atribuição, análise das ativações internas e explicações baseadas em exemplos, todas relevantes para entender a evolução das representações internas durante o fine-tuning. A integração dessas abordagens contribui para identificar limitações, corrigir vieses e aprimorar o desempenho dos modelos em tarefas que exigem alto grau de interpretabilidade.