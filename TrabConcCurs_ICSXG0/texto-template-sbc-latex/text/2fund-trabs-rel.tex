\section{Fundamentos e Trabalhos Relacionados}

Esta seção discute os trabalhos relacionados e os fundamentos teóricos principais relacionados com o tema proposto.

\subsection{LLMs - Large Language Models}

Os modelos de Linguagem de Grande Escala (\textit{Large Language Models} - \textit{LLMs}) são arquiteturas computacionais baseadas em redes neurais profundas, predominantemente do tipo \textit{transformer}, desenvolvidos para processar e gerar linguagem natural. Esses modelos são treinados com grandes volumes de dados textuais, com os quais conseguem (computar/aprender) padrões estatísticos da linaguagem humana, tornando-os capazes de realizar uma ampla gama de tarefas, grande parte sem treinamento supervisionado específico (e.g. sumarização, resposta a perguntas e tradução). A característica comumente associada ao tamanho desses modelos é uma medida em bilhções de parâmetros que está diretamente associado à sua capacidade de adaptação a contextos diversos e sua generalização. A arquitetura \textit{transformer}, introduzida por \cite{vaswani2023transformers}, faz parte da estrutura central da maioria dos LLMs atuais, e destaca-se pelo uso de mecanismos de atenção que possibilitam a modelagem de dependências contextuais de longo alcance. 

\subsection{Fine-tuning}

O \textit{fine-tunning} é uma técnica utilizada para adaptar \textit{LLMs} a tarefas específicas, por meio de re-treinamento de parte ou totalidade de seus parâmetros com conjuntos de dados mais específicos. Em seguida da etapa inicial de pré-treinamento, na qual um \textit{LLM} aprende as representações gerais linguagem a partir de grandes corpora, o \textit{fine-tunning} permite especializar o comportamento do modelo para contextos mais delimitados como, por exemplo, geração de código, classificação de sentimentos ou, como no presente trabalho, navegação por labirinto em caracteres ASCII. Existem diferentes estratégias de \textit{fine-tunning}:
\begin{itemize}
    \item Supervised Fine-Tuning (SFT): técnica de ajuste fino supervisionado, onde o modelo é treinado com um conjunto de dados rotulado, permitindo que aprenda a mapear entradas para saídas específicas. Essa abordagem é comum para tarefas como classificação, tradução e resposta a perguntas.
    \item Direct Preference Optimisation (DPO): técnica de otimização direta de preferências, onde o modelo é ajustado para maximizar a probabilidade de gerar respostas preferidas por humanos, com base em feedback direto. Essa abordagem é útil para melhorar a qualidade das respostas geradas pelo modelo.
    \item Reinforcement Learning from Human Feedback (RLHF): técnica de aprendizado por reforço a partir de feedback humano, onde o modelo é treinado para maximizar uma recompensa baseada em preferências humanas. Essa abordagem é eficaz para melhorar a qualidade das respostas geradas pelo modelo, alinhando-as com as expectativas dos usuários.
    \item Odds Ratio Preference Optimization (ORPO): técnica de otimização de preferências baseada em razão de chances, onde o modelo é ajustado para maximizar a razão entre as probabilidades de gerar respostas preferidas e não preferidas. Essa abordagem é útil para melhorar a qualidade das respostas geradas pelo modelo, alinhando-as com as expectativas dos usuários. 
    \item Group Relative Policy Optimization (GRPO): técnica de otimização de políticas relativas em grupo, onde o modelo é ajustado para maximizar a probabilidade de gerar respostas preferidas por um grupo de usuários, levando em consideração as preferências coletivas. Essa abordagem é eficaz para melhorar a qualidade das respostas geradas pelo modelo, alinhando-as com as expectativas de um público mais amplo.
\end{itemize}

\subsection{Explicabilidade em LLMs}

Apesar de possuírem um alto nível de compreensão e resolução daquilo que lhes são atribuídos, os LLMs são questionados com frequência sobre seu caráter de ``caixa-preta'', que é a dificuldade no entendimento sobre como tomam decisões ou gram suas respostas.

Nesse contexto, é cada vez maior o interesse em técnicas de explicabilidade (\textit{explainability}) aplicadas a redes neurais profundas para tornar seus mecanismos internos interpretáveis. A explicabilidade em LLMs tem o objetivo de revelar como os dados são processados ao longo das camadas do modelo, quais neurônios foram ativados em resposta a determinadas entradas, como essas ativações influenciam a saída final e como as representações internas evoluem durante o treinamento ou \textit{fine-tunning}.

Essas técnicas são essenciais para o diagnóstico de falhas, enviesamento e limitações de raciocínio do modelo. Diversas ferramentas vêm sendo propostas para auxiliar na explicabilidade de LLMs com visualizações, utilização de grafos de conhecimento, análise de atenção e outras abordagens que ajudam a decifrar o comportamento dos modelos.

\subsubsection{LLM-MRI}

A biblioteca LLM-MRI (\textit{Large Language Model - Magnetic Resonance Imaging}) é uma das ferramentas desenvolvidas com o intuito de auxiliar na explicabilidade de LLMs. Ela atua com o objetivo de simplificar o estudo de padrões de ativações em qualquer LLM baseado em trasnformer.

Desenvolvida por \cite{costa2024llmmri}, a LLM-MRI permite a coleta, organização e projeção em dimensões reduzidas dos vetores de ativação produzidos pelos modelos ao processarem diferentes entradas. Por meio de técnicas de redução de dimensionalidade, é possível visualizar como as ativações neuronais se distribuem em um espaço de menor dimensão, facilitando a identificação de padrões e também como as ativações se diferenciam após a aplicação de diferentes técnicas de fine-tunning.

Essa abordagem é relevante, considerando o contexto experimental deste trabalho, avaliar como LLMs se adaptam a tarefas específicas (neste caso, navegação em labirintos ASCII) e como as estruturas internas dos modelos emergem a partir dessa aprendizagem.

\subsection{Trabalhos Relacionados}

Esta seção apresenta a revisão de literatura sobre o tema deste trabalho, com foco em LLMs, fine-tunning e explicabilidade.

\subsubsection{Raciocínio Espacial em LLMs}

Entre as abordagens relevantes envolvendo o raciocínio espacial de LLMs, destaca-se o trabalho de \cite{dao2025alphamaze}, que introduz o framework AlphaMaze como uma estratégia, em duas etapas, para capacitar LLMs a resolverem labirintos representados em texto. Inicialmente, os modelos passam por um Supervised Fine-Tuning (SFT) com dados tokenizados para aprender comandos de movimentação passo a passo; em seguida, aplica-se o algoritmo Group Relative Policy Optimization (GRPO) para promover um raciocínio mais robusto e autocorretivo. O estudo apresenta também o MazeBench, usado para avaliar o desempenho dos modelos com labirintos de diferentes níveis de dificuldade, e demonstra um incremento significativo de desempenho após a aplicação do algoritmo GRPO.

De modo complementar, estudos como o de \cite{jiang2024supervised} questionam a capacidade de raciocício espacial em modelos multimodais, mostrando que modelos que processam apenas entradas textuais podem superar modelos multimodais em tarefas de raciocínio espacial. Para tarefas espaciais mais rigorosas, como o uso de LLMs para entendimento de espaços tridimensionais estruturados, \cite{zhang2025point} demonstrou que, apesar dos avanços significativos em outras áreas, LLMs ainda enfrentam desafios ao processar tais relações. Paralelamente, \cite{cheng2024spatialrgpt} desenvolve um framework que visa aprimorar a percepção e raciocínio tridimensional em modelos de linguagem visual, integrando módulos de representação de regiões e plugins para informações de profundidade, permitindo uma compreensão mais profunda das relações espaciais em cenas complexas.

Diferentemente de outras abordagens que requerem fine-tuning adicional, \cite{hu2023chain} introduz o método Chain-of-Symbol (COS) Prompting como uma técnica inovadora para aprimorar a capacidade de raciocínio espacial em LLMs. O COS Prompting converte descrições em linguagem natural para representações simbólicas durante as etapas intermediárias de raciocínio, permitindo que o modelo utilize símbolos de forma mais eficiente. Os resultados experimentais mostraram ganhos significativos em tarefas de planejamento espacial, incluindo reduções expressivas no uso de tokens e melhorias substanciais na acurácia em benchmarks específicos.

O método intitulado Visualization of Thought (VoT) também se destaca como uma abordagem alternativa para aprimorar o raciocínio espacial em LLMs. Inspirando-se na cognição humana, \cite{wu2024mind} propõe o VoT como uma técnica que estimula a criação de imagens mentais por parte dos modelos utilizando prompts específicos que guiam a geração de representações visuais em ASCII art durante a execução de tarefas, como resolução de labirintos e planejamento de caminhos. Experimentos realizados com o GPT-4 demonstraram que o VoT melhora a capacidade do modelo em acompanhar estados e planejar ações de forma mais precisa, superando abordagens tradicionais e modelos multimodais em determinados casos. Contudo, os resultados também indicam que essa abordagem depende fortemente das habilidades emergentes dos modelos e da qualidade dos prompts, mostrando limitações em cenários mais complexos ou com modelos menos robustos.

\subsubsection{Fine-tuning de LLMs para Tarefas Específicas}

O fine-tuning de LLMs para tarefas específicas é um tema amplamente investigado, com diferentes estratégias sendo propostas para aprimorar a adaptação dos modelos. Por exemplo, \cite{wang2024picture} demonstra que o uso de SFT pode melhorar tanto as representações internas quanto a capacidade de generalização, mesmo em modelos visuais, evidenciando a importância do ajuste direcionado para tarefas de raciocínio espacial. Além disso, \cite{hsieh2023distillingstepbystep} propõem a utilização de rationales gerados por LLMs como supervisão adicional em um cenário multitarefa, combinando rótulos e explicações para enriquecer o sinal de treinamento. Essa abordagem, baseada em Chain-of-Thought prompting \cite{wei2023chainofthought}, permite que modelos menores alcancem desempenho superior a LLMs maiores, mesmo com menos dados, ao tornar o processo de ajuste mais eficiente e interpretável. No contexto deste trabalho, a integração de racionalizações se mostra relevante para aprimorar a eficiência e a interpretabilidade dos modelos em tarefas como a resolução de labirintos ASCII.

\subsubsection{Explicabilidade em LLMs}

A explicabilidade em LLMs é um campo em expansão, com abordagens que buscam tornar os modelos mais transparentes e compreensíveis. A biblioteca LLM-MRI \cite{costa2024llmmri} destaca-se por permitir a análise e visualização das ativações neuronais, facilitando a interpretação de como os modelos se adaptam a tarefas específicas, como a navegação em labirintos ASCII. Complementarmente, \cite{wu2025usablexai10strategies} propõem o conceito de Usable XAI, que enfatiza a aplicação prática das explicações para diagnóstico e aprimoramento dos modelos. Entre as estratégias discutidas estão métodos de atribuição, análise das ativações internas e explicações baseadas em exemplos, todas relevantes para entender a evolução das representações internas durante o fine-tuning. A integração dessas abordagens contribui para identificar limitações, corrigir vieses e aprimorar o desempenho dos modelos em tarefas que exigem alto grau de interpretabilidade.