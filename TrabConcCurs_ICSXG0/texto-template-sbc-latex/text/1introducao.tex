\section{Introdução}

A resolução de labirintos é uma tarefa clássica que demanda raciocínio espacial, planejamento e adaptação a ambientes estruturados. Tradicionalmente, algoritmos simbólicos e métodos baseados em busca foram empregados para solucionar esse tipo de problema. No entanto, com o avanço dos Modelos de Linguagem de Grande Escala (\textit{Large Language Models} - LLMs), surge a oportunidade de investigar como essas arquiteturas, originalmente projetadas para linguagem natural, podem ser adaptadas para tarefas que exigem compreensão espacial e manipulação de representações simbólicas, como labirintos em ASCII.

Apesar do sucesso dos LLMs em tarefas linguísticas, ainda há lacunas no entendimento de como esses modelos internalizam e representam informações espaciais, especialmente após processos de especialização como o \textit{fine-tuning}. O caráter de ``caixa-preta'' dos LLMs dificulta a análise dos mecanismos internos responsáveis pelo aprendizado e pela tomada de decisão, tornando essencial o uso de técnicas de explicabilidade para investigar as transformações ocorridas nas redes neurais durante o treinamento para tarefas específicas.

Neste contexto, este trabalho propõe investigar como LLMs abertos, com até 8 bilhões de parâmetros, aprendem a resolver labirintos representados em ASCII por meio de diferentes estratégias de \textit{fine-tuning}. O objetivo central é analisar as mudanças nas ativações internas das redes neurais associadas ao aprendizado da tarefa, utilizando a biblioteca LLM-MRI \cite{costa2024llmmri} para visualizar e comparar as representações neuronais antes e depois do processo de aprendizagem. A análise busca identificar padrões emergentes, alterações estruturais e possíveis mecanismos de raciocínio espacial desenvolvidos pelos modelos.

Os principais objetivos específicos deste estudo são: (i) aplicar diferentes estratégias de \textit{fine-tuning} em LLMs abertos para a tarefa de resolução de labirintos em ASCII; (ii) avaliar o desempenho dos modelos antes e após o ajuste; (iii) utilizar a LLM-MRI para visualizar e comparar as ativações neuronais; (iv) investigar padrões emergentes e alterações nas representações internas; e (v) discutir as implicações dos resultados para o entendimento do raciocínio e da representação de tarefas específicas em LLMs.

Ao abordar essas questões, este trabalho busca contribuir tanto para o avanço prático das técnicas de especialização de LLMs quanto para o aprofundamento teórico sobre os processos de representação e raciocínio em redes neurais profundas, especialmente em contextos que exigem habilidades espaciais e interpretabilidade dos modelos.
