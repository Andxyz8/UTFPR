\section{Fundamentos e Trabalhos Relacionados}

Esta seção discute os trabalhos relacionados e os fundamentos teóricos principais relacionados com o tema proposto.

\subsection{LLMs - Large Language Models}

Os modelos de Linguagem de Grande Escala (\textit{Large Language Models} - \textit{LLMs}) são arquiteturas computacionais baseadas em redes neurais profundas, predominantemente do tipo \textit{transformer}, desenvolvidos para processar e gerar linguagem natural. Esses modelos são treinados com grandes volumes de dados textuais, com os quais conseguem (computar/aprender) padrões estatísticos da linaguagem humana, tornando-os capazes de realizar uma ampla gama de tarefas, grande parte sem treinamento supervisionado específico (e.g. sumarização, resposta a perguntas e tradução). A característica comumente associada ao tamanho desses modelos é uma medida em bilhções de parâmetros que está diretamente associado à sua capacidade de adaptação a contextos diversos e sua generalização. A arquitetura \textit{transformer}, introduzida por \cite{vaswani2023transformers}, faz parte da estrutura central da maioria dos LLMs atuais, e destaca-se pelo uso de mecanismos de atenção que possibilitam a modelagem de dependências contextuais de longo alcance. 

\subsection{Fine-tuning}

O \textit{fine-tunning} é uma técnica utilizada para adaptar \textit{LLMs} a tarefas específicas, por meio de re-treinamento de parte ou totalidade de seus parâmetros com conjuntos de dados mais específicos. Em seguida da etapa inicial de pré-treinamento, na qual um \textit{LLM} aprende as representações gerais linguagem a partir de grandes corpora, o \textit{fine-tunning} permite especializar o comportamento do modelo para contextos mais delimitados como, por exemplo, geração de código, classificação de sentimentos ou, como no presente trabalho, navegação por labirinto em caracteres ASCII. Existem diferentes estratégias de \textit{fine-tunning}:
\begin{itemize}
    \item Supervised Fine-Tuning (SFT): 
    \item Direct Preference Optimisation (DPO): 
    \item Reinforcement Learning from Human Feedback (RLHF): 
    \item Odds Ratio Preference Optimization (ORPO): 
\end{itemize}

\subsection{Explicabilidade em LLMs}

Apesar de possuírem um alto nível de compreensão e resolução daquilo que lhes são atribuídos, os LLMs são questionados com frequência sobre seu caráter de ``caixa-preta'', que é a dificuldade no entendimento sobre como tomam decisões ou gram suas respostas.

Nesse contexto, é cada vez maior o interesse em técnicas de explicabilidade (\textit{explainability}) aplicadas a redes neurais profundas para tornar seus mecanismos internos interpretáveis. A explicabilidade em LLMs tem o objetivo de revelar como os dados são processados ao longo das camadas do modelo, quais neurônios foram ativados em resposta a determinadas entradas, como essas ativações influenciam a saída final e como as representações internas evoluem durante o treinamento ou \textit{fine-tunning}.

Essas técnicas são essenciais para o diagnóstico de falhas, enviesamento e limitações de raciocínio do modelo. Diversas ferramentas vêm sendo propostas para auxiliar na explicabilidade de LLMs com visualizações, utilização de grafos de conhecimento, análise de atenção e outras abordagens que ajudam a decifrar o comportamento dos modelos.

\subsubsection{LLM-MRI}

A biblioteca LLM-MRI (\textit{Large Language Model - Magnetic Resonance Imaging}) é uma das ferramentas desenvolvidas com o intuito de auxiliar na explicabilidade de LLMs. Ela atua com o objetivo de simplificar o estudo de padrões de ativações em qualquer LLM baseado em trasnformer.

Desenvolvida por \cite{costa2024llmmri}, a LLM-MRI permite a coleta, organização e projeção em dimensões reduzidas dos vetores de ativação produzidos pelos modelos ao processarem diferentes entradas. Por meio de técnicas de redução de dimensionalidade, é possível visualizar como as ativações neuronais se distribuem em um espaço de menor dimensão, facilitando a identificação de padrões e também como as ativações se diferenciam após a aplicação de diferentes técnicas de fine-tunning.

Essa abordagem é relevante, considerando o contexto experimental deste trabalho, avaliar como LLMs se adaptam a tarefas específicas (neste caso, navegação em labirintos ASCII) e como as estruturas internas dos modelos emergem a partir dessa aprendizagem.

\subsection{Trabalhos Relacionados}

Esta seção apresenta a revisão de literatura sobre o tema deste trabalho, com foco em LLMs, fine-tunning e explicabilidade.

\subsubsection{Raciocínio Espacial em LLMs}

Entre as abordagens relevantes envolvendo o raciocínio espacial de LLMs, destaca-se o trabalho de \cite{dao2025alphamaze}, que introduz o framework AlphaMaze como uma estratégia, em duas etapas, para capacitar LLMs a resolverem labirintos representados em texto. Inicialmente, os modelos passam por um Supervised Fine-Tuning (SFT) com dados tokenizados para aprender comandos de movimentação passo a passo; em seguida, aplica-se o algoritmo Group Relative Policy Optimization (GRPO) para promover um raciocínio mais robusto e autocorretivo. O estudo apresenta também o MazeBench, usado para avaliar o desempenho dos modelos com labirintos de diferentes níveis de dificuldade, e demonstra um incremento significativo de desempenho após a aplicação do algoritmo GRPO.

De modo complementar, estudos como o de \cite{jiang2024supervised} questionam a capacidade de raciocício espacial em modelos multimodais, mostrando que modelos que processam apenas entradas textuais podem superar modelos multimodais em tarefas de raciocínio espacial. Além disso, o trabalho de \cite{wang2024picture} destaca como a técnica de \textit{fine-tunning} supervisionado, mesmo em modelos visuais, pode aprimorar suas representações internas e a capacidade de generalização, reforçando a importância do treinamento específico para tarefas de raciocínio espacial. Para tarefas espaciais mais rigorosas, como o uso de LLMs para entendimento de espaços tridimensionais estruturados, o trabalho de \cite{algo} demonstrou que, apesar dos avanços significativos em outras áreas, LLMs ainda enfrentam desafios ao processar tais relações.